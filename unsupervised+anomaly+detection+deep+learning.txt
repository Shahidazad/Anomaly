from google.colab import drive
drive.mount('/content/drive’)

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import tensorflow as tf
from tensorflow.keras import layers
import time

import os
os.chdir("drive/My Drive/Anomaly")

ls

# Load the data
data = pd.read_csv("Anomaly dataset for deep learning.csv")
data.head()

#label encoding 
for column in data.columns:
    if data[column].dtype == np.object:
        encoded = LabelEncoder()
        
        encoded.fit(data[column])
        data[column] = encoded.transform(data[column])
data.head()

### Checking Null values
list1 = []
for i in data.columns:
  null = sum(pd.isnull(data[i]))
  null1 = i+' - '+str(null)
  list1.append(null1)
list1

# Preprocess the data
mean = np.mean(data, axis=0)
std = np.std(data, axis=0)
data = (data - mean) / std
data.head()

# Build the autoencoder model
input_layer = tf.keras.layers.Input(shape=(data.shape[1],))
encoded = tf.keras.layers.Dense(128, activation="relu")(input_layer)
encoded = tf.keras.layers.Dense(64, activation="relu")(encoded)
decoded = tf.keras.layers.Dense(128, activation="relu")(encoded)
decoded = tf.keras.layers.Dense(data.shape[1], activation="sigmoid")(decoded)
autoencoder = tf.keras.models.Model(input_layer, decoded)
autoencoder.compile(optimizer="adam", loss="mean_squared_error")

# Train the autoencoder
autoencoder.fit(data, data, epochs=100, batch_size=64)

# Calculate the reconstruction error for each sample
predictions = autoencoder.predict(data)
reconstruction_error = np.mean((predictions - data)**2, axis=1)

# Threshold the reconstruction error to identify anomalies
threshold = np.mean(reconstruction_error) + 3 * np.std(reconstruction_error)
anomalies = np.where(reconstruction_error > threshold)
anomalies

anomaly_data = data.iloc[[i for i in anomalies[0]]]
anomaly_data

#-----------------------------------------------------------------------------------
# Build the generator model - GAN
generator = tf.keras.Sequential()
generator.add(tf.keras.layers.Dense(64, input_dim=100, activation="relu"))
generator.add(tf.keras.layers.Dense(128, activation="relu"))
generator.add(tf.keras.layers.Dense(data.shape[1], activation="sigmoid"))

# Build the discriminator model
discriminator = tf.keras.Sequential()
discriminator.add(tf.keras.layers.Dense(128, input_dim=data.shape[1], activation="relu"))
discriminator.add(tf.keras.layers.Dense(64, activation="relu"))
discriminator.add(tf.keras.layers.Dense(1, activation="sigmoid"))

# Compile the discriminator
discriminator.compile(optimizer="adam", loss="binary_crossentropy")
# Freeze the discriminator weights
discriminator.trainable = False

# Build the combined model
inputs = tf.keras.layers.Input(shape=(100,))
generated_data = generator(inputs)
validity = discriminator(generated_data)
gan = tf.keras.models.Model(inputs, validity)
gan.compile(optimizer="adam", loss="binary_crossentropy")

# Train the GAN
for epoch in range(100):
    # Generate synthetic data
    noise = np.random.normal(0, 1, (data.shape[0], 100))
    synthetic_data = generator.predict(noise)
    # Train the discriminator on real data and synthetic data
    real_labels = np.ones((data.shape[0], 1))
    synthetic_labels = np.zeros((data.shape[0], 1))
    d_loss_real = discriminator.train_on_batch(data, real_labels)
    d_loss_synthetic = discriminator.train_on_batch(synthetic_data, synthetic_labels)
    # Train the generator
    g_loss = gan.train_on_batch(noise, real_labels)
    # Print the loss values for each epoch
    print("Epoch: %d, D Loss (real): %.4f, D Loss (synthetic): %.4f, G Loss: %.4f" % (epoch + 1, d_loss_real, d_loss_synthetic, g_loss))

# Evaluate the synthetic data using the discriminator
scores = discriminator.predict(data)
# Threshold the scores to identify anomalies
threshold = np.mean(scores) + 3 * np.std(scores)
anomalies = np.where(reconstruction_error > threshold)

anomaly_data = data.iloc[[i for i in anomalies[0]]]
anomaly_data

#------------------------------------------------------------------------------------------
# Define the DBN model
model = tf.keras.models.Sequential()

# Add RBMs as hidden layers
model.add(tf.keras.layers.BatchNormalization(input_shape=(37,)))
model.add(tf.keras.layers.Dense(32, activation='sigmoid', name="layer1"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(16, activation='sigmoid', name="layer2"))
model.add(tf.keras.layers.BatchNormalization())

# Add the output layer
model.add(tf.keras.layers.Dense(37, activation='sigmoid', name="output"))
# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy')
# Fit the model to the data
model.fit(data, data, epochs=50)
# Use the model to reconstruct the input data
reconstructed_data = model.predict(data)

# Compute the reconstruction error
error = np.mean(np.abs(data - reconstructed_data))
# Choose a threshold for the reconstruction error
threshold = 0.1
# Identify anomalies
anomalies = np.abs(data - reconstructed_data) > threshold

# Print the anomalies
print(data[anomalies])

